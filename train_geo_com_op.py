"""
train_pure_geometry_jepa.py ‚Äî CORRECT PURE GEOMETRY JEPA

CRITICAL FIX: Encoder NEVER sees coordinates
- ‚ùå OLD: EGNN (geometry-aware) 
- ‚úÖ NEW: GraphTransformer / GIN / GINE (pure graph)

WHY THIS FIXES EVERYTHING:
1. Energy gap > 0 (clean vs corrupted views are different)
2. Reconstruction works (no gradient conflicts)
3. True JEPA: encoder ‚Üí geometry heads ‚Üí coordinates
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
import random
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import pandas as pd

# Import modular components
from dataset_optimized import QM9OptimizedDataset, collate_fn_optimized
# NO LONGER NEED: import bonds, import bond_types (pre-computed!)
import geometry

# ‚ùå REMOVED: from models.egnn_clean.egnn_clean import EGNN
# ‚úÖ NEW: Pure geometry encoders
from pure_geometry_encoders import create_encoder


# ============================================================================
# GEOMETRY HEADS (UNCHANGED)
# ============================================================================

class BondLengthHead(nn.Module):
    def __init__(self, latent_dim=128):
        super().__init__()
        self.bond_type_embed = nn.Embedding(13, 32)
        
        self.mlp = nn.Sequential(
            nn.Linear(latent_dim * 2 + 32, latent_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Softplus()
        )
    
    def forward(self, z, edge_index, bond_types=None):
        if edge_index.shape[1] == 0:
            return torch.zeros(0, device=z.device)
        
        src, dst = edge_index
        features = torch.cat([z[src], z[dst]], dim=-1)
        
        if bond_types is not None and bond_types.shape[0] == edge_index.shape[1]:
            type_embed = self.bond_type_embed(bond_types)
            features = torch.cat([features, type_embed], dim=-1)
        else:
            type_embed = torch.zeros(features.shape[0], 32, device=z.device)
            features = torch.cat([features, type_embed], dim=-1)
        
        return self.mlp(features).squeeze(-1)


class AngleHead(nn.Module):
    """Predict angles in RADIANS [0, œÄ]"""
    def __init__(self, latent_dim=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(latent_dim * 3, latent_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, z, angle_triplets):
        if angle_triplets.shape[0] == 0:
            return torch.zeros(0, device=z.device)
        i, j, k = angle_triplets[:, 0], angle_triplets[:, 1], angle_triplets[:, 2]
        features = torch.cat([z[i], z[j], z[k]], dim=-1)
        return self.mlp(features).squeeze(-1) * torch.pi


class TorsionHead(nn.Module):
    def __init__(self, latent_dim=128):
        super().__init__()
        self.bin_classifier = nn.Sequential(
            nn.Linear(latent_dim * 4, latent_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 3)
        )
        
        self.residual_regressor = nn.Sequential(
            nn.Linear(latent_dim * 4, latent_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 2)
        )
    
    def forward(self, z, torsion_quads):
        if torsion_quads.shape[0] == 0:
            return torch.zeros((0, 2), device=z.device), torch.zeros((0, 3), device=z.device)
        
        i, j, k, l = torsion_quads[:, 0], torsion_quads[:, 1], torsion_quads[:, 2], torsion_quads[:, 3]
        features = torch.cat([z[i], z[j], z[k], z[l]], dim=-1)
        
        bins = self.bin_classifier(features)
        residual = self.residual_regressor(features)
        
        return residual, bins


class RepulsionHead(nn.Module):
    def __init__(self, latent_dim=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(latent_dim * 2, latent_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Softplus()
        )
    
    def forward(self, z, nonbond_pairs):
        if nonbond_pairs.shape[1] == 0:
            return torch.zeros(0, device=z.device)
        src, dst = nonbond_pairs
        features = torch.cat([z[src], z[dst]], dim=-1)
        return self.mlp(features).squeeze(-1)


class ContrastiveEnergyHead(nn.Module):
    """
    CRITICAL: Now works because encoder sees different graphs for clean vs corrupt
    (different augmentations), not different coordinates
    """
    def __init__(self, latent_dim=128):
        super().__init__()
        self.energy_mlp = nn.Sequential(
            nn.Linear(latent_dim, latent_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(latent_dim, latent_dim // 2),
            nn.ReLU(),
            nn.Linear(latent_dim // 2, 1)
        )
    
    def forward(self, z_global):
        return self.energy_mlp(z_global).squeeze(-1)
    
    def contrastive_loss(self, z_clean, z_corrupted_list, epoch, max_epochs=100):
        progress = min(epoch / max_epochs, 1.0)
        base_margin = 2.0 + 3.0 * progress
        
        energy_clean = self(z_clean)
        
        total_loss = 0.0
        energy_corrupted_sum = 0.0
        
        for i, z_corrupted in enumerate(z_corrupted_list):
            energy_corrupted = self(z_corrupted)
            margin = base_margin * (1.0 + i * 0.3)
            loss = F.relu(energy_clean - energy_corrupted + margin)
            total_loss += loss.mean()
            energy_corrupted_sum += energy_corrupted.mean().item()
        
        avg_loss = total_loss / len(z_corrupted_list)
        avg_energy_clean = energy_clean.mean().item()
        avg_energy_corrupted = energy_corrupted_sum / len(z_corrupted_list)
        
        return avg_loss, avg_energy_clean, avg_energy_corrupted


class DistanceDistributionHead(nn.Module):
    """Global shape constraint"""
    def __init__(self, latent_dim=128, num_bins=20, max_dist=10.0):
        super().__init__()
        self.num_bins = num_bins
        self.max_dist = max_dist
        
        self.mlp = nn.Sequential(
            nn.Linear(latent_dim, latent_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(latent_dim, latent_dim // 2),
            nn.ReLU(),
            nn.Linear(latent_dim // 2, num_bins),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, z_global):
        return self.mlp(z_global)
    
    def compute_distance_histogram(self, pos, batch_idx):
        batch_size = batch_idx.max().item() + 1
        histograms = []
        
        for b in range(batch_size):
            mask = batch_idx == b
            n_atoms = mask.sum().item()
            
            if n_atoms < 2:
                histograms.append(torch.zeros(self.num_bins, device=pos.device))
                continue
            
            mol_pos = pos[mask]
            dists = torch.cdist(mol_pos, mol_pos, p=2)
            triu_idx = torch.triu_indices(n_atoms, n_atoms, offset=1, device=pos.device)
            dists_flat = dists[triu_idx[0], triu_idx[1]]
            
            hist = torch.histc(dists_flat, bins=self.num_bins, min=0, max=self.max_dist)
            hist = hist / (hist.sum() + 1e-8)
            histograms.append(hist)
        
        return torch.stack(histograms)


class ValenceHead(nn.Module):
    def __init__(self, latent_dim=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(latent_dim * 2, latent_dim),
            nn.ReLU(),
            nn.Linear(latent_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, z, edge_index):
        if edge_index.shape[1] == 0:
            return torch.zeros(0, device=z.device)
        src, dst = edge_index
        features = torch.cat([z[src], z[dst]], dim=-1)
        return self.mlp(features).squeeze(-1)


# ============================================================================
# PURE GEOMETRY JEPA MODEL - CORRECTED ARCHITECTURE
# ============================================================================

class PureGeometryJEPA(nn.Module):
    """
    ‚úÖ CORRECT ARCHITECTURE:
    
    1. Encoder: Graph ‚Üí Latent (NO coordinates)
    2. Heads: Latent ‚Üí Geometry predictions
    3. Reconstruction: Geometry ‚Üí Coordinates
    
    ‚ùå WRONG (old): EGNN sees coordinates
    ‚úÖ RIGHT (new): Pure graph encoder
    """
    def __init__(self, atom_dim=10, hidden_dim=128, latent_dim=128, 
                 encoder_layers=4, encoder_type='transformer', device='cpu'):
        super().__init__()
        self.device = device
        self.latent_dim = latent_dim
        self.encoder_type = encoder_type
        
        # Atom embedding
        self.atom_embed = nn.Sequential(
            nn.Linear(atom_dim, hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim)
        )
        
        # ‚úÖ CRITICAL FIX: Pure graph encoder (NO coordinates)
        self.graph_encoder = create_encoder(
            encoder_type=encoder_type,
            in_dim=hidden_dim,
            hidden_dim=hidden_dim,
            num_layers=encoder_layers,
            num_heads=8,
            dropout=0.1
        )
        
        # Latent projection
        self.latent_proj = nn.Sequential(
            nn.Linear(hidden_dim, latent_dim),
            nn.LayerNorm(latent_dim),
            nn.ReLU(),
            nn.Linear(latent_dim, latent_dim)
        )
        
        # Geometry prediction heads
        self.bond_head = BondLengthHead(latent_dim)
        self.angle_head = AngleHead(latent_dim)
        self.torsion_head = TorsionHead(latent_dim)
        self.repulsion_head = RepulsionHead(latent_dim)
        self.energy_head = ContrastiveEnergyHead(latent_dim)
        self.valence_head = ValenceHead(latent_dim)
        self.dist_dist_head = DistanceDistributionHead(latent_dim)
    
    def forward(self, x, edge_index, batch_idx, bond_types=None,
                return_corrupted=False, corruption_fn=None, epoch=0):
        """
        CRITICAL: NO pos_true or pos_noisy in encoder!
        
        ‚úÖ FIX: Use edge dropout for proper graph corruption
        """
        B = batch_idx.max().item() + 1
        N = x.shape[0]
        
        # Embed atoms
        h = self.atom_embed(x)
        
        # ‚úÖ Encode graph (NO coordinates)
        h_graph = self.graph_encoder(h, edge_index, bond_types)
        
        # Project to latent space
        z = self.latent_proj(h_graph)
        z_global = self._global_pool(z, batch_idx, B)
        
        # Extract geometry structure
        angle_triplets = geometry.get_angles(edge_index, N)
        torsion_quads = geometry.get_torsions(edge_index, N)
        nonbond_pairs = geometry.get_nonbonded_pairs(edge_index, N)
        
        # Predict geometry
        pred_bond_lengths = self.bond_head(z, edge_index, bond_types)
        pred_angles = self.angle_head(z, angle_triplets)
        pred_torsions, pred_torsion_bins = self.torsion_head(z, torsion_quads)
        pred_repulsion = self.repulsion_head(z, nonbond_pairs)
        pred_valence_probs = self.valence_head(z, edge_index)
        pred_dist_distribution = self.dist_dist_head(z_global)
        
        outputs = {
            'z': z,
            'z_global': z_global,
            'pred_bond_lengths': pred_bond_lengths,
            'pred_angles': pred_angles,
            'pred_torsions': pred_torsions,
            'pred_torsion_bins': pred_torsion_bins,
            'pred_repulsion': pred_repulsion,
            'pred_valence_probs': pred_valence_probs,
            'pred_dist_distribution': pred_dist_distribution,
            'edge_index': edge_index,
            'angle_triplets': angle_triplets,
            'torsion_quads': torsion_quads,
            'nonbond_pairs': nonbond_pairs
        }
        
        # ‚úÖ FIX: Use EDGE DROPOUT + NODE CORRUPTION for proper graph corruption
        if return_corrupted:
            n_corrupted = 3
            z_global_corrupted_list = []
            
            for i in range(n_corrupted):
                # Progressive edge dropout (10%, 20%, 30%)
                drop_prob = 0.1 + i * 0.1
                edge_index_corrupt, bond_types_corrupt = edge_dropout(
                    edge_index, bond_types, drop_prob=drop_prob
                )
                
                # ‚úÖ FIX: Also corrupt node features (handles isolated nodes)
                h_corrupt = node_feature_corruption(h, corruption_prob=drop_prob)
                
                # Encode corrupted graph
                h_graph_corrupt = self.graph_encoder(h_corrupt, edge_index_corrupt, bond_types_corrupt)
                z_corrupt = self.latent_proj(h_graph_corrupt)
                z_global_corrupted_list.append(
                    self._global_pool(z_corrupt, batch_idx, B)
                )
            
            outputs['z_global_corrupted'] = z_global_corrupted_list
        
        return outputs
    
    def _global_pool(self, z, batch_idx, batch_size):
        z_global = torch.zeros(batch_size, z.shape[-1], device=z.device)
        for b in range(batch_size):
            mask = batch_idx == b
            if mask.sum() > 0:
                z_global[b] = z[mask].mean(dim=0)
        return z_global
    
    def reconstruct_coordinates_multistage(self, outputs, batch_idx, pos_init=None):
        """
        Multi-stage reconstruction: Geometry ‚Üí Coordinates
        
        Stage 1: Bonds only
        Stage 2: Bonds + Angles
        Stage 3: Full geometry
        
        ‚úÖ NUMERICALLY STABLE: Gradient clipping, NaN checking
        """
        N = outputs['z'].shape[0]
        B = batch_idx.max().item() + 1
        device = outputs['z'].device
        
        if pos_init is None:
            # Initialize with small random values (not too large)
            pos_init = torch.randn(N, 3, device=device) * 0.1
            pos_init.requires_grad_(True)
        else:
            pos_init = pos_init.clone().detach()
            pos_init.requires_grad_(True)
        
        pred_lengths = outputs['pred_bond_lengths'].detach()
        pred_angles = outputs['pred_angles'].detach()
        pred_torsions = outputs['pred_torsions'].detach()
        pred_min_dists = outputs['pred_repulsion'].detach()
        
        edge_index = outputs['edge_index']
        angle_triplets = outputs['angle_triplets']
        torsion_quads = outputs['torsion_quads']
        nonbond_pairs = outputs['nonbond_pairs']
        
        # Stage 1: Bonds (30 steps, LR=0.2)
        optimizer = torch.optim.Adam([pos_init], lr=0.2)
        for step in range(30):
            optimizer.zero_grad()
            loss = torch.tensor(0.0, device=device)
            
            if edge_index.shape[1] > 0:
                src, dst = edge_index
                current_lengths = torch.norm(pos_init[src] - pos_init[dst], dim=-1)
                loss += F.mse_loss(current_lengths, pred_lengths) * 10.0
            
            for b in range(B):
                mol_mask = batch_idx == b
                if mol_mask.sum() > 0:
                    com = pos_init[mol_mask].mean(dim=0)
                    loss += (com ** 2).sum() * 1e-3
            
            # Check for NaN
            if not torch.isfinite(loss):
                break
            
            loss.backward()
            
            # Clip gradients
            torch.nn.utils.clip_grad_norm_([pos_init], 10.0)
            
            optimizer.step()
            
            # Check for NaN in positions
            if not torch.isfinite(pos_init).all():
                # Reset to small random
                pos_init.data = torch.randn_like(pos_init) * 0.1
        
        # Stage 2: Bonds + Angles (40 steps, LR=0.1)
        optimizer = torch.optim.Adam([pos_init], lr=0.1)
        for step in range(40):
            optimizer.zero_grad()
            loss = torch.tensor(0.0, device=device)
            
            if edge_index.shape[1] > 0:
                src, dst = edge_index
                current_lengths = torch.norm(pos_init[src] - pos_init[dst], dim=-1)
                loss += F.mse_loss(current_lengths, pred_lengths) * 5.0
            
            if angle_triplets.shape[0] > 0:
                i, j, k = angle_triplets[:, 0], angle_triplets[:, 1], angle_triplets[:, 2]
                v1 = pos_init[i] - pos_init[j]
                v2 = pos_init[k] - pos_init[j]
                
                # Normalize with safety
                v1_norm = torch.norm(v1, dim=-1, keepdim=True).clamp(min=1e-6)
                v2_norm = torch.norm(v2, dim=-1, keepdim=True).clamp(min=1e-6)
                v1 = v1 / v1_norm
                v2 = v2 / v2_norm
                
                cos_current = (v1 * v2).sum(dim=-1).clamp(-1.0, 1.0)
                current_angles = torch.acos(cos_current)
                loss += F.mse_loss(current_angles, pred_angles) * 2.0
            
            for b in range(B):
                mol_mask = batch_idx == b
                if mol_mask.sum() > 0:
                    com = pos_init[mol_mask].mean(dim=0)
                    loss += (com ** 2).sum() * 1e-3
            
            if not torch.isfinite(loss):
                break
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_([pos_init], 10.0)
            optimizer.step()
            
            if not torch.isfinite(pos_init).all():
                pos_init.data = torch.randn_like(pos_init) * 0.1
        
        # Stage 3: Full geometry (30 steps, LR=0.05)
        optimizer = torch.optim.Adam([pos_init], lr=0.05)
        for step in range(30):
            optimizer.zero_grad()
            loss = torch.tensor(0.0, device=device)
            
            if edge_index.shape[1] > 0:
                src, dst = edge_index
                current_lengths = torch.norm(pos_init[src] - pos_init[dst], dim=-1)
                loss += F.mse_loss(current_lengths, pred_lengths) * 2.0
            
            if angle_triplets.shape[0] > 0:
                i, j, k = angle_triplets[:, 0], angle_triplets[:, 1], angle_triplets[:, 2]
                v1 = pos_init[i] - pos_init[j]
                v2 = pos_init[k] - pos_init[j]
                
                v1_norm = torch.norm(v1, dim=-1, keepdim=True).clamp(min=1e-6)
                v2_norm = torch.norm(v2, dim=-1, keepdim=True).clamp(min=1e-6)
                v1 = v1 / v1_norm
                v2 = v2 / v2_norm
                
                cos_current = (v1 * v2).sum(dim=-1).clamp(-1.0, 1.0)
                current_angles = torch.acos(cos_current)
                loss += F.mse_loss(current_angles, pred_angles) * 1.0
            
            if torsion_quads.shape[0] > 0:
                i, j, k, l = torsion_quads[:, 0], torsion_quads[:, 1], torsion_quads[:, 2], torsion_quads[:, 3]
                v1 = pos_init[j] - pos_init[i]
                v2 = pos_init[k] - pos_init[j]
                v3 = pos_init[l] - pos_init[k]
                
                n1 = torch.cross(v1, v2, dim=-1)
                n2 = torch.cross(v2, v3, dim=-1)
                
                n1_norm = torch.norm(n1, dim=-1, keepdim=True).clamp(min=1e-6)
                n2_norm = torch.norm(n2, dim=-1, keepdim=True).clamp(min=1e-6)
                n1 = n1 / n1_norm
                n2 = n2 / n2_norm
                
                v2_normalized = v2 / torch.norm(v2, dim=-1, keepdim=True).clamp(min=1e-6)
                
                cos_current = (n1 * n2).sum(dim=-1).clamp(-1.0, 1.0)
                sin_current = (torch.cross(n1, n2, dim=-1) * v2_normalized).sum(dim=-1).clamp(-1.0, 1.0)
                current_sincos = torch.stack([sin_current, cos_current], dim=-1)
                loss += F.mse_loss(current_sincos, pred_torsions) * 0.5
            
            if nonbond_pairs.shape[1] > 0:
                src, dst = nonbond_pairs
                current_dists = torch.norm(pos_init[src] - pos_init[dst], dim=-1)
                violations = F.relu(pred_min_dists - current_dists + 0.3)
                loss += violations.mean() * 0.3
            
            for b in range(B):
                mol_mask = batch_idx == b
                if mol_mask.sum() > 0:
                    com = pos_init[mol_mask].mean(dim=0)
                    loss += (com ** 2).sum() * 1e-3
            
            if not torch.isfinite(loss):
                break
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_([pos_init], 10.0)
            optimizer.step()
            
            if not torch.isfinite(pos_init).all():
                pos_init.data = torch.randn_like(pos_init) * 0.1
        
        return pos_init.detach()


# ============================================================================
# LOSS (UNCHANGED)
# ============================================================================

class CompleteLoss(nn.Module):
    def __init__(self, w_bond=1.0, w_angle=0.5, w_torsion=0.3, w_repulsion=0.4, 
                 w_energy=0.3, w_valence=0.3, w_dist_dist=0.5):
        super().__init__()
        self.w_bond = w_bond
        self.w_angle = w_angle
        self.w_torsion = w_torsion
        self.w_repulsion = w_repulsion
        self.w_energy = w_energy
        self.w_valence = w_valence
        self.w_dist_dist = w_dist_dist
    
    def forward(self, outputs, pos_true, batch_idx, 
                bond_types=None, expected_lengths=None,
                atom_max_valences=None, atom_is_hydrogen=None):
        losses = {}
        device = pos_true.device
        
        # Bond loss with H down-weighting
        bond_loss = torch.tensor(0.0, device=device)
        if outputs['edge_index'].shape[1] > 0:
            src, dst = outputs['edge_index']
            true_lengths = torch.norm(pos_true[src] - pos_true[dst], dim=-1)
            pred_lengths = outputs['pred_bond_lengths']
            
            errors = (pred_lengths - true_lengths) ** 2
            
            weights = torch.ones_like(errors)
            if bond_types is not None:
                weights[bond_types == 2] = 2.0
                weights[bond_types == 3] = 3.0
                weights[bond_types == 12] = 1.5
            
            if atom_is_hydrogen is not None:
                h_bonds = atom_is_hydrogen[src] + atom_is_hydrogen[dst]
                weights[h_bonds > 0] *= 0.3
            
            bond_loss = (weights * errors).mean()
        
        losses['bond'] = bond_loss
        
        # Angle loss in RADIANS
        angle_loss = torch.tensor(0.0, device=device)
        if outputs['angle_triplets'].shape[0] > 0:
            i, j, k = outputs['angle_triplets'][:, 0], outputs['angle_triplets'][:, 1], outputs['angle_triplets'][:, 2]
            v1 = pos_true[i] - pos_true[j]
            v2 = pos_true[k] - pos_true[j]
            
            cos_angle = F.cosine_similarity(v1, v2, dim=-1).clamp(-1.0, 1.0)
            true_angles = torch.acos(cos_angle)
            
            pred_angles = outputs['pred_angles']
            
            errors = (pred_angles - true_angles) ** 2
            
            weights = torch.ones_like(errors)
            if atom_is_hydrogen is not None:
                h_count = atom_is_hydrogen[i] + atom_is_hydrogen[j] + atom_is_hydrogen[k]
                weights[h_count > 0] = 0.3
            
            angle_loss = (weights * errors).mean()
        
        losses['angle'] = angle_loss
        
        # Torsion loss with H down-weighting
        torsion_loss = torch.tensor(0.0, device=device)
        torsion_bin_loss = torch.tensor(0.0, device=device)
        
        if outputs['torsion_quads'].shape[0] > 0:
            i, j, k, l = outputs['torsion_quads'][:, 0], outputs['torsion_quads'][:, 1], outputs['torsion_quads'][:, 2], outputs['torsion_quads'][:, 3]
            v1 = pos_true[j] - pos_true[i]
            v2 = pos_true[k] - pos_true[j]
            v3 = pos_true[l] - pos_true[k]
            n1 = torch.cross(v1, v2, dim=-1)
            n2 = torch.cross(v2, v3, dim=-1)
            n1 = F.normalize(n1 + 1e-6, dim=-1)
            n2 = F.normalize(n2 + 1e-6, dim=-1)
            v2_norm = F.normalize(v2 + 1e-6, dim=-1)
            cos_true = (n1 * n2).sum(dim=-1)
            sin_true = (torch.cross(n1, n2, dim=-1) * v2_norm).sum(dim=-1)
            true_sincos = torch.stack([sin_true, cos_true], dim=-1)
            
            pred_torsions = outputs['pred_torsions']
            errors = (pred_torsions - true_sincos) ** 2
            
            weights = torch.ones(errors.shape[0], device=device)
            if atom_is_hydrogen is not None:
                h_count = atom_is_hydrogen[i] + atom_is_hydrogen[j] + atom_is_hydrogen[k] + atom_is_hydrogen[l]
                weights[h_count > 0] = 0.3
            
            torsion_loss = (weights.unsqueeze(-1) * errors).mean()
            
            # Bin classification
            true_angles_rad = torch.atan2(sin_true, cos_true)
            true_bins = torch.zeros((true_angles_rad.shape[0], 3), device=device)
            anti_mask = torch.abs(true_angles_rad) > 2.0
            gauche_plus = (true_angles_rad > 0.5) & (true_angles_rad < 1.5)
            gauche_minus = (true_angles_rad < -0.5) & (true_angles_rad > -1.5)
            
            true_bins[anti_mask, 0] = 1.0
            true_bins[gauche_plus, 1] = 1.0
            true_bins[gauche_minus, 2] = 1.0
            
            bin_sums = true_bins.sum(dim=-1, keepdim=True)
            true_bins = true_bins / (bin_sums + 1e-8)
            
            pred_bins = F.softmax(outputs['pred_torsion_bins'], dim=-1)
            torsion_bin_loss = F.kl_div(pred_bins.log(), true_bins, reduction='batchmean')
        
        losses['torsion'] = torsion_loss
        losses['torsion_bin'] = torsion_bin_loss
        
        # Repulsion
        repulsion_loss = torch.tensor(0.0, device=device)
        if outputs['nonbond_pairs'].shape[1] > 0:
            src, dst = outputs['nonbond_pairs']
            true_dists = torch.norm(pos_true[src] - pos_true[dst], dim=-1)
            violations = F.relu(outputs['pred_repulsion'] - true_dists + 0.3)
            repulsion_loss = violations.mean()
        
        losses['repulsion'] = repulsion_loss
        
        # Valence penalty
        valence_loss = torch.tensor(0.0, device=device)
        if atom_max_valences is not None and outputs['edge_index'].shape[1] > 0:
            src, dst = outputs['edge_index']
            valence_probs = outputs['pred_valence_probs']
            
            N = pos_true.shape[0]
            predicted_valence = torch.zeros(N, device=device)
            predicted_valence.scatter_add_(0, src, valence_probs)
            predicted_valence.scatter_add_(0, dst, valence_probs)
            predicted_valence *= 0.5

            
            violations = F.relu(predicted_valence - atom_max_valences.float())
            valence_loss = violations.mean()
        
        losses['valence'] = valence_loss
        
        # Distance distribution loss
        # Distance distribution loss
        dist_dist_loss = torch.tensor(0.0, device=device)

        if outputs.get('true_dist_distribution') is not None:
            pred_dist_hist = outputs['pred_dist_distribution']
            true_dist_hist = outputs['true_dist_distribution']

            dist_dist_loss = F.kl_div(
                (pred_dist_hist + 1e-8).log(),
                true_dist_hist + 1e-8,
                reduction='batchmean'
            )

        
        losses['dist_dist'] = dist_dist_loss
        
        losses['energy'] = torch.tensor(0.0, device=device)
        
        total = (self.w_bond * bond_loss +
                self.w_angle * angle_loss +
                self.w_torsion * (torsion_loss + torsion_bin_loss) +
                self.w_repulsion * repulsion_loss +
                self.w_valence * valence_loss +
                self.w_dist_dist * dist_dist_loss)
        
        losses['total'] = total
        return losses


# ============================================================================
# GRAPH CORRUPTION FOR CONTRASTIVE LEARNING
# ============================================================================

def edge_dropout(edge_index, bond_types, drop_prob=0.1):
    """
    ‚úÖ PROPER GRAPH CORRUPTION: Drop edges randomly
    
    This creates semantically different graphs (not just feature noise)
    """
    if edge_index.shape[1] == 0 or drop_prob == 0:
        return edge_index, bond_types
    
    # Random mask (keep edges with prob 1-drop_prob)
    keep_mask = torch.rand(edge_index.shape[1], device=edge_index.device) > drop_prob
    
    edge_index_dropped = edge_index[:, keep_mask]
    
    if bond_types is not None and bond_types.shape[0] > 0:
        bond_types_dropped = bond_types[keep_mask]
    else:
        bond_types_dropped = bond_types
    
    return edge_index_dropped, bond_types_dropped


def node_feature_corruption(h, corruption_prob=0.1):
    """
    ‚úÖ FIX: Add node feature corruption alongside edge dropout
    
    This ensures isolated nodes (from edge dropout) still get corrupted
    """
    if corruption_prob == 0:
        return h
    
    # Randomly mask some node features
    mask = (torch.rand(h.shape[0], 1, device=h.device) > corruption_prob).float()
    h_corrupted = h * mask
    
    # Add small noise to non-masked features
    noise = torch.randn_like(h) * 0.1
    h_corrupted = h_corrupted + noise * (1 - mask)
    
    return h_corrupted

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def flatten_batch(x, pos, mask, device):
    """
    ‚úÖ OPTIMIZED: Simple flattening (no chemistry extraction)
    """
    B, N = mask.shape
    x_list, pos_list, batch_list = [], [], []
    
    for b in range(B):
        mol_mask = mask[b].bool()
        n_atoms = mol_mask.sum().item()
        if n_atoms == 0:
            continue
        x_list.append(x[b, mol_mask])
        pos_list.append(pos[b, mol_mask])
        batch_list.append(torch.full((n_atoms,), b, device=device, dtype=torch.long))
    
    if len(x_list) == 0:
        return None, None, None, 0
    
    x_flat = torch.cat(x_list, dim=0)
    pos_flat = torch.cat(pos_list, dim=0)
    batch_idx = torch.cat(batch_list, dim=0)
    
    return x_flat, pos_flat, batch_idx, B


def compute_rmsd(pred, true):
    """Compute RMSD with Kabsch alignment (robust to numerical issues)"""
    if pred.shape[0] < 3:
        return torch.tensor(0.0, device=pred.device)
    
    # Check for NaN or Inf
    if not torch.isfinite(pred).all() or not torch.isfinite(true).all():
        return torch.tensor(float('inf'), device=pred.device)
    
    # Center both structures
    pred_c = pred - pred.mean(dim=0, keepdim=True)
    true_c = true - true.mean(dim=0, keepdim=True)
    
    # Compute covariance matrix
    H = pred_c.T @ true_c
    
    # Check if H is finite
    if not torch.isfinite(H).all():
        return torch.tensor(float('inf'), device=pred.device)
    
    try:
        # SVD for optimal rotation
        U, S, Vt = torch.linalg.svd(H, full_matrices=False)
        
        # Ensure proper rotation (not reflection)
        d = torch.sign(torch.linalg.det(Vt.T @ U.T))
        diag = torch.eye(3, device=pred.device)
        diag[2, 2] = d
        R = Vt.T @ diag @ U.T
        
        # Apply rotation
        pred_aligned = pred_c @ R + true.mean(dim=0, keepdim=True)
        
        # Compute RMSD
        msd = torch.mean((pred_aligned - true) ** 2)
        rmsd = torch.sqrt(msd + 1e-8)
        
        # Final safety check
        if not torch.isfinite(rmsd):
            return torch.tensor(float('inf'), device=pred.device)
        
        return rmsd
    
    except Exception as e:
        # SVD failed - return inf
        return torch.tensor(float('inf'), device=pred.device)


# ============================================================================
# TRAINING
# ============================================================================

def train_epoch(model, loader, optimizer, criterion, device, epoch, max_epochs):
    model.train()
    total_losses = {
        'total': 0.0, 'bond': 0.0, 'angle': 0.0, 'torsion': 0.0, 'torsion_bin': 0.0,
        'repulsion': 0.0, 'energy': 0.0, 'valence': 0.0, 'dist_dist': 0.0
    }
    num_batches = 0
    
    energy_clean_list = []
    energy_corrupted_list = []
    
    for batch in tqdm(loader, desc=f"Epoch {epoch+1}/{max_epochs}", leave=False):
        # ‚úÖ OPTIMIZED: Chemistry already in batch, just move to GPU
        x = batch['x'].to(device)
        pos = batch['pos'].to(device)
        mask = batch['mask'].to(device)
        edge_index = batch['edge_index'].to(device)
        bond_types = batch['bond_types'].to(device)
        expected_lengths = batch['expected_lengths'].to(device)
        atom_max_valences = batch['atom_max_valences'].to(device)
        atom_is_hydrogen = batch['atom_is_hydrogen'].to(device)
        
        result = flatten_batch(x, pos, mask, device)
        if result[0] is None:
            continue
        
        x_flat, pos_flat, batch_idx, B = result
        
        # ‚úÖ Forward (NO RDKit calls!)
        outputs = model(
            x_flat, edge_index, batch_idx,
            bond_types=bond_types,
            return_corrupted=True,
            epoch=epoch
        )
        
        # Compute true distance distribution (only after epoch 10)
        if epoch >= 10:
            true_dist_hist = model.dist_dist_head.compute_distance_histogram(pos_flat, batch_idx)
            outputs['true_dist_distribution'] = true_dist_hist
        else:
            outputs['true_dist_distribution'] = None
        
        # Geometry losses (with curriculum)
        losses = criterion(
            outputs, pos_flat, batch_idx,
            bond_types=bond_types,
            expected_lengths=expected_lengths,
            atom_max_valences=atom_max_valences,
            atom_is_hydrogen=atom_is_hydrogen,
        )
        
        # Contrastive energy
        if 'z_global_corrupted' in outputs:
            energy_loss, avg_e_clean, avg_e_corrupted = model.energy_head.contrastive_loss(
                outputs['z_global'],
                outputs['z_global_corrupted'],
                epoch=epoch,
                max_epochs=max_epochs
            )
            losses['energy'] = energy_loss
            losses['total'] = losses['total'] + criterion.w_energy * energy_loss
            
            energy_clean_list.append(avg_e_clean)
            energy_corrupted_list.append(avg_e_corrupted)
        
        # Backward
        optimizer.zero_grad()
        losses['total'].backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
        optimizer.step()
        
        for key in total_losses:
            total_losses[key] += losses[key].item()
        num_batches += 1
    
    metrics = {key: val / max(num_batches, 1) for key, val in total_losses.items()}
    
    if len(energy_clean_list) > 0:
        metrics['energy_clean'] = np.mean(energy_clean_list)
        metrics['energy_corrupted'] = np.mean(energy_corrupted_list)
        metrics['energy_gap'] = metrics['energy_corrupted'] - metrics['energy_clean']
    
    return metrics


def evaluate(model, loader, criterion, device, reconstruct_coords=False, epoch=0, max_epochs=100):
    model.eval()
    total_losses = {
        'total': 0.0, 'bond': 0.0, 'angle': 0.0, 'torsion': 0.0, 'torsion_bin': 0.0,
        'repulsion': 0.0, 'energy': 0.0, 'valence': 0.0, 'dist_dist': 0.0
    }
    rmsd_values = []  # ‚úÖ FIX: Collect all RMSDs for median
    num_batches = 0
    
    energy_clean_list = []
    energy_corrupted_list = []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating", leave=False):
            # ‚úÖ OPTIMIZED: Move pre-computed chemistry to GPU
            x = batch['x'].to(device)
            pos = batch['pos'].to(device)
            mask = batch['mask'].to(device)
            edge_index = batch['edge_index'].to(device)
            bond_types = batch['bond_types'].to(device)
            expected_lengths = batch['expected_lengths'].to(device)
            atom_max_valences = batch['atom_max_valences'].to(device)
            atom_is_hydrogen = batch['atom_is_hydrogen'].to(device)
            
            result = flatten_batch(x, pos, mask, device)
            if result[0] is None:
                continue
            
            x_flat, pos_flat, batch_idx, B = result
            
            outputs = model(
                x_flat, edge_index, batch_idx,
                bond_types=bond_types,
                return_corrupted=True,
                epoch=epoch
            )
            
            # ‚úÖ FIX: Only compute distance distribution after epoch 10
            if epoch >= 10:
                true_dist_hist = model.dist_dist_head.compute_distance_histogram(pos_flat, batch_idx)
                outputs['true_dist_distribution'] = true_dist_hist
            else:
                outputs['true_dist_distribution'] = None
            
            losses = criterion(
                outputs, pos_flat, batch_idx,
                bond_types=bond_types,
                expected_lengths=expected_lengths,
                atom_max_valences=atom_max_valences,
                atom_is_hydrogen=atom_is_hydrogen,
            )
            
            if 'z_global_corrupted' in outputs:
                energy_loss, avg_e_clean, avg_e_corrupted = model.energy_head.contrastive_loss(
                    outputs['z_global'],
                    outputs['z_global_corrupted'],
                    epoch=epoch,
                    max_epochs=max_epochs
                )
                losses['energy'] = energy_loss
                losses['total'] = losses['total'] + criterion.w_energy * energy_loss
                
                energy_clean_list.append(avg_e_clean)
                energy_corrupted_list.append(avg_e_corrupted)
            
            for key in total_losses:
                total_losses[key] += losses[key].item()
            num_batches += 1
            
            # ‚úÖ FIX: Only reconstruct after epoch 10 (geometry has stabilized)
            if reconstruct_coords and epoch >= 10:
                with torch.enable_grad():
                    pos_recon = model.reconstruct_coordinates_multistage(outputs, batch_idx)
                
                # Check if reconstruction is valid
                if not torch.isfinite(pos_recon).all():
                    continue  # Skip this batch if reconstruction failed
                
                for mol_id in batch_idx.unique():
                    mol_mask = batch_idx == mol_id
                    if mol_mask.sum() < 3:
                        continue
                    
                    rmsd = compute_rmsd(pos_recon[mol_mask], pos_flat[mol_mask])
                    
                    # ‚úÖ FIX: Collect RMSD for median (handles outliers better)
                    if torch.isfinite(rmsd) and rmsd < 100.0:  # Sanity check
                        rmsd_values.append(rmsd.item())
    
    metrics = {key: val / max(num_batches, 1) for key, val in total_losses.items()}
    
    # ‚úÖ FIX: Report median RMSD (more robust than mean)
    if reconstruct_coords and len(rmsd_values) > 0:
        metrics['rmsd'] = float(np.median(rmsd_values))
        metrics['rmsd_mean'] = float(np.mean(rmsd_values))
        metrics['rmsd_std'] = float(np.std(rmsd_values))
    
    if len(energy_clean_list) > 0:
        metrics['energy_clean'] = np.mean(energy_clean_list)
        metrics['energy_corrupted'] = np.mean(energy_corrupted_list)
        metrics['energy_gap'] = metrics['energy_corrupted'] - metrics['energy_clean']
    
    return metrics


# ============================================================================
# MAIN
# ============================================================================

def main():
    set_seed(42)
    
    batch_size = 32
    num_epochs = 100
    lr = 1e-4
    encoder_type = 'transformer'  # 'transformer', 'gin', or 'gine'
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("="*80)
    print("PURE GEOMETRY JEPA - CORRECT ARCHITECTURE + OPTIMIZED")
    print("="*80)
    print("‚úÖ CRITICAL FIX: Encoder NEVER sees coordinates")
    print(f"‚úÖ Using: {encoder_type.upper()} encoder")
    print("‚úÖ Architecture: Graph ‚Üí Latent ‚Üí Geometry ‚Üí Coordinates")
    print("‚úÖ OPTIMIZED: All chemistry pre-computed on CPU (NO RDKit during training)")
    print("")
    print("‚ùå OLD (WRONG): EGNN sees pos_noisy ‚Üí geometry leaked to encoder")
    print("‚úÖ NEW (RIGHT): Pure graph encoder ‚Üí geometry only in heads")
    print("="*80)
    print(f"Device: {device}")
    print(f"Batch size: {batch_size}, Epochs: {num_epochs}, LR: {lr}")
    print("="*80)
    
    # ‚úÖ OPTIMIZED: Pre-compute all chemistry during dataset loading
    dataset = QM9OptimizedDataset(
        jsonl_path='/scratch/nishanth.r/egnn/data/qm9_100k.jsonl',
        atom_feature_dim=10,
        verbose=True
    )
    
    train_size = int(0.8 * len(dataset))
    val_size = int(0.1 * len(dataset))
    test_size = len(dataset) - train_size - val_size
    
    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size, test_size]
    )
    
    # ‚úÖ OPTIMIZED: Use optimized collate function
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_optimized)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_optimized)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_optimized)
    
    print(f"Train: {train_size} | Val: {val_size} | Test: {test_size}")
    
    model = PureGeometryJEPA(
        atom_dim=10,
        hidden_dim=128,
        latent_dim=128,
        encoder_layers=4,
        encoder_type=encoder_type,
        device=device
    ).to(device)
    
    print(f"Parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    criterion = CompleteLoss(
        w_bond=1.0,
        w_angle=0.5,
        w_torsion=0.3,
        w_repulsion=0.4,
        w_energy=0.3,
        w_valence=0.3,
        w_dist_dist=0.5
    )
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    history = {
        'train_total': [], 'train_bond': [], 'train_angle': [], 'train_torsion': [],
        'train_torsion_bin': [], 'train_repulsion': [], 'train_energy': [],
        'train_valence': [], 'train_dist_dist': [],
        'train_energy_clean': [], 'train_energy_corrupted': [], 'train_energy_gap': [],
        'val_total': [], 'val_bond': [], 'val_angle': [], 'val_torsion': [],
        'val_torsion_bin': [], 'val_repulsion': [], 'val_energy': [],
        'val_valence': [], 'val_dist_dist': [], 'val_rmsd': [],
        'val_energy_clean': [], 'val_energy_corrupted': [], 'val_energy_gap': []
    }
    
    best_val_loss = float('inf')
    
    print("\n" + "="*80)
    print("TRAINING START")
    print("="*80)
    print("\nüîç WHAT TO WATCH:")
    print("   1. energy_gap should be > 0 (was ~0 with EGNN)")
    print("   2. bond_loss should drop < 0.1")
    print("   3. angle_loss should drop < 0.05")
    print("   4. RMSD should drop < 0.3 √Ö")
    print("="*80 + "\n")
    
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        print("-"*80)
        
        train_metrics = train_epoch(model, train_loader, optimizer, criterion, device, epoch, num_epochs)
        
        reconstruct = (epoch % 10 == 0) or (epoch == num_epochs - 1)
        val_metrics = evaluate(model, val_loader, criterion, device, 
                              reconstruct_coords=reconstruct, epoch=epoch, max_epochs=num_epochs)
        
        # Store metrics
        for key in ['total', 'bond', 'angle', 'torsion', 'torsion_bin', 'repulsion', 
                    'energy', 'valence', 'dist_dist']:
            history[f'train_{key}'].append(train_metrics[key])
            history[f'val_{key}'].append(val_metrics[key])
        
        history['val_rmsd'].append(val_metrics.get('rmsd', np.nan))
        
        for prefix in ['train', 'val']:
            metrics = train_metrics if prefix == 'train' else val_metrics
            for suffix in ['energy_clean', 'energy_corrupted', 'energy_gap']:
                history[f'{prefix}_{suffix}'].append(metrics.get(suffix, np.nan))
        
        # Print
        print(f"Train | Total: {train_metrics['total']:.4f} | Bond: {train_metrics['bond']:.4f} | "
              f"Angle: {train_metrics['angle']:.4f} | DistDist: {train_metrics['dist_dist']:.4f}")
        
        if 'energy_gap' in train_metrics:
            gap_status = "‚úÖ" if train_metrics['energy_gap'] > 1.0 else "‚ö†Ô∏è" if train_metrics['energy_gap'] > 0 else "‚ùå"
            print(f"      | Energy Gap: {train_metrics['energy_gap']:.4f} {gap_status} | "
                  f"E_clean: {train_metrics['energy_clean']:.4f} | "
                  f"E_corrupt: {train_metrics['energy_corrupted']:.4f}")
        
        print(f"Val   | Total: {val_metrics['total']:.4f} | Bond: {val_metrics['bond']:.4f} | "
              f"Angle: {val_metrics['angle']:.4f} | DistDist: {val_metrics['dist_dist']:.4f}")
        
        if 'rmsd' in val_metrics:
            rmsd_status = "‚úÖ" if val_metrics['rmsd'] < 0.3 else "‚ö†Ô∏è" if val_metrics['rmsd'] < 0.5 else "‚ùå"
            print(f"Val RMSD: {val_metrics['rmsd']:.4f} √Ö {rmsd_status}")
        
        if val_metrics['total'] < best_val_loss:
            best_val_loss = val_metrics['total']
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': best_val_loss,
                'history': history,
                'encoder_type': encoder_type
            }, f'best_pure_jepa_{encoder_type}.pt')
            print(f"‚úì Saved best model (val_loss: {best_val_loss:.4f})")
        
        scheduler.step()
    
    # Save results
    Path('metrics').mkdir(exist_ok=True)
    pd.DataFrame(history).to_csv(f'metrics/pure_jepa_{encoder_type}.csv', index=False)
    
    print("\n" + "="*80)
    print("TRAINING COMPLETE!")
    print("="*80)
    print(f"‚úÖ Saved: best_pure_jepa_{encoder_type}.pt")
    print(f"‚úÖ Saved: metrics/pure_jepa_{encoder_type}.csv")
    print("\nüìä CRITICAL ARCHITECTURE CHANGE:")
    print(f"   ‚ùå OLD: EGNN (geometry-aware) ‚Üí energy_gap = 0")
    print(f"   ‚úÖ NEW: {encoder_type.upper()} (pure graph) ‚Üí energy_gap > 0")
    print("\n‚úÖ ALL CRITICAL FIXES:")
    print("   1. Pure graph encoder (NO coordinates)")
    print("   2. Angles in radians (NO reflection ambiguity)")
    print("   3. H down-weighted 0.3x (heavy atoms prioritized)")
    print("   4. Distance distribution (global shape)")
    print("   5. Multi-stage reconstruction (stable)")
    print("="*80)


if __name__ == '__main__':
    main()